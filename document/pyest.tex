\documentclass[12pt,preprint]{aastex}
\usepackage{amssymb,amsmath}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\this}{\project{PyEST}}
\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\fig}[1]{Figure \ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\eq}[1]{Equation \ref{eq:#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

% math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\normal}[3]{\mathcal{N} (#1 | #2, #3)}

% model parameters
\newcommand{\model}{\paramvector{\Theta}}
\newcommand{\data}{\paramvector{X}}

% units
\newcommand{\unit}[1]{\mathrm{#1}}

\begin{document}

\title{PyEST: Python Ensemble Sampling Toolkit}
\author{Daniel~Foreman-Mackey\altaffilmark{1}}
\affil{Center for Cosmology and Particle Physics,
                         Department of Physics, New York University,
                         4 Washington Place, New York, NY, 10003, USA; danfm@nyu.edu}

\begin{abstract}

    Markov chain Monte Carlo (MCMC) has proven to be a powerful tool for Bayesian
    parameter estimation in many applications in astronomy and cosmology.  Most
    standard MCMC algorithms require fine-tuning of the proposal distribution
    through a computationally expensive ``burn-in'' phase --- especially when the
    sampled distribution is highly covariant.
    Affine invariant ensemble samplers don't face this problem and they generally
    converge with fewer steps in the chain.

    We present a generalization to the \citet{Goodman:2010} affine invariant sampler
    that allows for arbitrarily sophisticated unsupervised machine learning techniques
    to be used to generate the proposal distribution.  We show that even a very simple
    application of our algorithm is extremely efficient.  We judge the efficiency
    of the algorithm based on the autocorrelation time of many chains.

    This result is particularly useful in situations where the likelihood call is
    very computationally expensive so that the extra cost associated with generating
    the proposal is outweighed by the reduction in autocorrelation time.

\end{abstract}

\keywords{
}

\section{Introduction}

\begin{enumerate}

    \item Bayesian inference  --- and with it, MCMC sampling --- is becoming an
        integral part of the day-to-day routine for many astronomers (tons of
        citations). and "why you might want to join the ranks of these kool kids;
        it's not hard, and you don't have to drink the kool-aid to be Bayesian
        when necessary." Remember, a maximum-likelihood problem can be rephrased
        as an MCMC problem easily by assuming flat priors.

    \item Traditional sampling methods (M-H) become extremely inefficient as the
        underlying distribution becomes non-Gaussian or even just highly covariant.
        There have been various ad-hoc fine-tuning methods proposed to deal with
        this problem (citations; annealing, proposal covariance estimation, etc.)
        and various more sophisticated methods (?; e.g.~Nested sampling, etc.;
        citations) but these modifications are often difficult to implement and
        require significant fine-tuning of free parameters to obtain an efficient
        sampling.

    \item Cite Goodman \& Weare and give a quick summary of why it's good.

    \item Cite Hou \etal, Lang \& Hogg and any other applications.

\end{enumerate}

\section{The Algorithm}

\citet{Hou:2011}

At the core of \this, lies the affine invariant ensemble sampler from
\citep{Goodman:2010}.  In particular, we have implemented the ``stretch move''
algorithm.  To achieve convergence with standard MCMC samplers, several
hyperparameters must be fine tuned to account for non-trivial covariances between
the model parameters of a particular problem \citep[e.g.][]{Dunkley:2005}.
The performance of an affine invariant sampler, however, is independent of the
off-diagonal terms in the covariance matrix of any specific distribution.

\subsection{The stretch move}



\begin{enumerate}

    \item Summarize the Goodman \& Weare algorithm here trying to draw attention
        to what it means to have an affine invariant ensemble and why an astronomer
        might care. Maybe include a cartoon?

\end{enumerate}

\section{Our Implementation}

\begin{enumerate}

    \item Why Python?

    \item ``Checkerboard'' stepping instead of sequential in order to parallelize

    \item Sample code

    \item Access, license, etc.

\end{enumerate}

\section{Numerical Tests}

\begin{enumerate}

    \item Outline a basic M-H algorithm as the baseline comparison

    \item Introduce autocorrelation time as an analytic for convergence testing.

    \item Tests:
        \begin{itemize}
            \item \emph{N}-Gaussian (if proposal distribution isn't chosen properly
                M-H is slow but it's fine if the proposal distribution is right).
                \this~destroys this problem no matter what!

            \item Rosenbrock density (highly non-Gaussian) --- \emph{epic} failure
                for M-H but \this~does well.

            \item Multimodal distribution?

            \item Fitting a line?

            \item Others?
        \end{itemize}

\end{enumerate}

\section{Discussion}

\begin{enumerate}

    \item Discuss specific applications of the algorithm in astronomy (Hou \etal;
        Hogg \& Lang; DFM \& Widrow; \etc)

    \item Convince everybody that they should use it and cite us\ldots

\end{enumerate}

\begin{thebibliography}{70}
\raggedright

\bibitem[Dunkley \etal(2005)]{Dunkley:2005}
{Dunkley}, J., {Bucher}, M., {Ferreira}, P.~G., {Moodley}, K., \& {Skordis}, C.,
2005, \mnras, 356, 925-936
% http://adsabs.harvard.edu/abs/2005MNRAS.356..925D

\bibitem[Goodman \& Weare~(2010)]{Goodman:2010}
Goodman,~J., \& Weare,~J.,
2010, Comm.\ App.\ Math.\ Comp.\ Sci., 5, 65

\bibitem[F. Hou \etal(2011))]{Hou:2011}
{Hou}, F., {Goodman}, J., {Hogg}, D.~W., {Weare}, J., \& {Schwab}, C.,
2011, arXiv:1104.2612
% http://adsabs.harvard.edu/abs/2011arXiv1104.2612H

\end{thebibliography}


\end{document}


